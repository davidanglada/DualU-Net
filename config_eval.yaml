dataset:
  train:
    name: consep
    root: "/mnt/work/users/david.anglada/datasets/consep_coco/"  # Path to the training dataset
    fold: Train
    num_classes: 4                   # Fold to use for training
  val:
    name: consep
    root: "/mnt/work/users/david.anglada/datasets/consep_coco/"    # Path to the validation dataset
    fold: Test
    num_classes: 4                   # Fold to use for validation
  test:
    name: consep
    root: "/mnt/work/users/david.anglada/datasets/consep_coco/"    # Path to the test dataset
    fold: Test
    num_classes: 4                   # Fold to use for testing

loader:
  train:
    shuffle: True
    batch_size: 1
    num_workers: 1
    drop_last: False
  val:
    shuffle: False
    batch_size: 4
    num_workers: 1
    drop_last: False
  test:
    shuffle: False
    batch_size: 1
    num_workers: 1
    drop_last: False

optimizer:
  lr_base: 0.0001
  epochs: 100
  weight_decay: 0.0001
  lr_drop_steps:
    - 70
    - 90
  lr_drop_factor: 0.1
  lr_auto_scale: True

distributed: True

transforms:
  sigma: 4
  normalize:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  augmentations:
    - name: "hflip"
      p: 0.5
    - name: "vflip"
      p: 0.5
    - name: "rotate90"

training:
  loss_seg: "combined"
  ce_weights: ./ce_weights_consep_Train.npy
  sigma: 5
  th: 0.15
  weight_ce: 1.0
  weight_dice: 1.0
  weight_mse: 1.0

model:
  encoder_name: resnext50_32x4d 
  classes_s: 4                   
  classes_c: 1                    
  encoder_weights: imagenet      
  decoder_channels: [256, 128, 64, 32, 16] 
  decoder_use_batchnorm: True

evaluation:
  thresholds: [0.5]
  max_pair_distance: 12
  interval: 1   

logging:
  use_wandb: True
  wandb_project: "Your_Project_Name"
  wandb_run_name: "checkpoint.pth" 
  

experiment:
  wandb: True
  project: "Your_Project_Name"
  name: "checkpoint.pth" 
  wandb_group: "test"
  seed: 42
  output_dir: /path/to/output_dir 
  output_name: "checkpoint.pth" 
  resume: False
